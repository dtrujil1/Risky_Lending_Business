---
title: "Data Preparation & Cleaning"
output: github_document
---

```{r setup, include=F}
knitr::opts_chunk$set(
  fig.path = "markdown_figs/data_preparation-"
)
```

```{r message=F}
library(magrittr)
library(dplyr)
library(ggplot2)
library(glmnet)
library(randomForest)
library(caret)
library(data.table)
library(doParallel)
library(klaR)
```

## Loading the dataset

```{r echo=FALSE}
loan_df <- fread("data/train_v3.csv", data.table = F, colClasses = c("character"))
#test_scenario1_2_dt <- fread("data/test_scenario1_2.csv")
#test_scenario3_dt <- fread("data/test_scenario3.csv")
```

## Cleaning The Data Up

### Recoding The Predictor Variables

```{r}
loan_df <- loan_df %>% mutate_all(as.numeric)
```

```{r}
head(loan_df)
```

### Recoding The Target Variable

```{r}
target_var <- loan_df %>%
  dplyr::select(loss) %>%
  mutate_at(c("loss"), ~ifelse(. > 0, 1, 0)) %>%
  mutate_at(c("loss"), as.factor)
```

### Removing Near Zero Variance Variables

```{r}
cluster <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cluster)

nzv_cols <- nearZeroVar(loan_df, allowParallel = T, foreach = T)
loan_df_cln <- loan_df[, -nzv_cols]

stopCluster(cluster)
registerDoSEQ()
```

### Quick Inputation of `NA`s

```{r}
anyNA(loan_df_cln)
```

```{r}
impute_model <- preProcess(loan_df_cln, method = "medianImpute")
loan_df_cln2 <- predict(impute_model, loan_df_cln)
```

### Preparing The Data for Feature Selection

```{r}
loan_df_cln3 <- loan_df_cln2 %>% dplyr::select(-c("V1", "id"))

predictor_vars <- data.matrix(loan_df_cln3)

loss <- target_var %>% use_series("loss") %>% as.vector()
```

## Feature Selection and Extraction

### Selecting Features using LASSO

```{r}
set.seed(123)
cluster <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(cluster)

cv_lasso <- cv.glmnet(
  predictor_vars, loss,
  type.measure = "auc",
  family = "binomial",
  alpha = 1,
  parallel = T
)

stopCluster(cluster)
registerDoSEQ()
```

```{r}
plot(cv_lasso)
```

```{r}
lasso_model <- glmnet(
  predictor_vars, loss,
  lambda = cv_lasso$lambda.min,
  standardize = T,
  family = "binomial",
  alpha = 1
)
```

```{r}
coeff_matrix <- coef(lasso_model, s = "lambda.min")

coeff_matrix
```

```{r}
lasso_coefs <- data.frame(
  name = coeff_matrix@Dimnames[[1]][coeff_matrix@i + 1],
  coefficient = coeff_matrix@x
  ) %>%
  filter(name !="(Intercept)") %>%
  arrange(-abs(coefficient)) %>%
  use_series("name") %>%
  as.character()

lasso_coefs
```

#### Separating Good Predictor Variables from the Noise

```{r}
good_predictors <- loan_df_cln3 %>% dplyr::select(lasso_coefs)

head(good_predictors)
```

#### Applying PCA to Compresss Good Predictor Variables

```{r}
pca_model <- preProcess(
  good_predictors,
  method = c( "center", "scale", "pca")
)

pca_df <- predict(pca_model, good_predictors)

head(pca_df)
```

#### Reducing Uncorrelated Variables < 90% correlation

```{r}
cor_matrix <- cor(good_predictors)

# Find attributes that are highly corrected
high_corr_vars <- findCorrelation(cor_matrix, cutoff = 0.90, names = T)

print(high_corr_vars)
```

```{r}
good_predictors_rm_hcorr <- good_predictors %>% dplyr::select(-high_corr_vars)

head(good_predictors_rm_hcorr)
```


```{r}
saveRDS(cv_lasso, file = "data/cv_lasso_auc.rds")
save(good_predictors, good_predictors_rm_hcorr, pca_df, loss, file = "data/data_preparation-v2.RData")
```
